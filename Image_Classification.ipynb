{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment"
      ],
      "metadata": {
        "id": "rF__moss_rmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.1. What is a Convolutional Neural Network (CNN), and how does it differ from\n",
        "traditional fully connected neural networks in terms of architecture and performance on\n",
        "image data?\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "**Convolutional Neural Network :**\n",
        "\n",
        "A Convolutional Neural Network (CNN) is a deep learning model designed primarily for processing grid-like data such as images, using convolutional layers to automatically extract hierarchical features like edges, textures, and objects.\n",
        "\n",
        "**Architecture Differences :**\n",
        "CNNs employ convolutional layers with sliding filters (kernels) that apply shared weights across local regions of the input, reducing parameters compared to fully connected neural networks (FCNNs), where every neuron connects to all neurons in the prior layer. Pooling layers in CNNs further downsample feature maps for efficiency and translation invariance, while FCNNs lack spatial awareness and treat inputs as flat vectors. CNNs often end with fully connected layers for classification, blending local feature extraction with global decisions.\n",
        "\n",
        "**Performance on Images Diffrerences :**\n",
        "CNNs outperform FCNNs on image data due to fewer parameters, enabling faster training and lower overfitting risk on high-dimensional inputs like 224x224x3 images, which would require millions of weights in FCNNs. Parameter sharing and local connectivity in CNNs capture spatial hierarchies effectively, unlike FCNNs' sensitivity to pixel shifts without positional context."
      ],
      "metadata": {
        "id": "hu8xhZGh_rjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.2. Discuss the architecture of LeNet-5 and explain how it laid the foundation\n",
        "for modern deep learning models in computer vision. Include references to its original\n",
        "research paper.\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "LeNet-5 is a pioneering convolutional neural network architecture introduced in 1998 by Yann LeCun and colleagues, designed for handwritten digit recognition, particularly the MNIST dataset. It contains seven layers excluding the input layer, comprising a mix of convolutional layers, subsampling (pooling) layers, and fully connected layers, establishing the foundational structure for modern CNNs.\n",
        "\n",
        "**Architecture of LeNet-5**\n",
        "\n",
        "The network features:\n",
        "\n",
        "1. Convolutional layers (C1 and C3): C1 has 6 feature maps created by applying 5x5 filters to the input image (32x32 pixels) generating 28x28 feature maps. C3 has 16 feature maps that connect selectively to previous layer maps to encourage feature diversity.\n",
        "\n",
        "2. Subsampling layers (S2 and S4): These perform average pooling with 2x2 windows and stride 2 to reduce spatial size and improve robustness to distortions.\n",
        "\n",
        "3. Convolutional layer (C5): This layer has 120 feature maps connected in a fully connected manner but using convolution filters, yielding 1x1 outputs.\n",
        "\n",
        "4. Fully connected layers (F6 and Output): The F6 layer has 84 units, and the output layer applies softmax to classify digits into 10 categories.\n",
        "\n",
        "**Foundation for Modern Deep Learning :**\n",
        "\n",
        "\n",
        "LeNet-5's architecture laid the groundwork by showing that automatic feature extraction through learned filters can outperform hand-engineered features in computer vision. Its core principles of convolutional layers, parameter sharing, local receptive fields, and pooling inspired later models like AlexNet and ResNet. This architecture demonstrated how hierarchical feature learning enables efficient training and accurate recognition on visual tasks.\n",
        "\n",
        "**Reference to Original Research Paper :**\n",
        "\n",
        "Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner\n",
        "“Gradient-Based Learning Applied to Document Recognition”\n",
        "Proceedings of the IEEE, 86(11): 2278–2324, 1998."
      ],
      "metadata": {
        "id": "FNr5VRr7_rZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.3. Compare and contrast AlexNet and VGGNet in terms of design principles,\n",
        "number of parameters, and performance. Highlight key innovations and limitations of\n",
        "each.\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "**Design Principles :**\n",
        "\n",
        "AlexNet uses larger 11x11 and 5x5 convolutional filters in its 8-layer structure, incorporates ReLU activations for faster training, overlapping pooling, and data augmentation like random cropping to combat overfitting. VGGNet emphasizes simplicity and depth with uniform 3x3 filters across 16 (VGG16) or 19 (VGG19) layers, relying on stacked small convolutions to approximate larger receptive fields while maintaining spatial resolution through stride-1 convolutions.\n",
        "\n",
        "**Parameters and Performance :**\n",
        "\n",
        "AlexNet has about 60 million parameters, balancing depth and compute for top-5 ImageNet error of 15.3%. VGGNet variants reach 138 million (VGG19), yielding better top-5 error around 7% but demanding far more training time and memory (over 500MB model size).\n",
        "\n",
        "**Key Innovations and Limitations :**\n",
        "\n",
        "- AlexNet innovations: Pioneered GPU parallelization (split across two GPUs), dropout in fully connected layers, and demonstrated deep CNN viability beyond LeNet. Limitations: Fewer layers limit feature hierarchy depth; larger kernels increase early computation.​\n",
        "\n",
        "- VGGNet innovations: Uniform architecture enables easy scaling and transfer learning; proved deeper networks with small filters boost accuracy. Limitations: Excessive depth causes vanishing gradients without advanced techniques; naive stacking ignores efficiency.​"
      ],
      "metadata": {
        "id": "9hCNkS2U_rVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.4. What is transfer learning in the context of image classification? Explain\n",
        "how it helps in reducing computational costs and improving model performance with\n",
        "limited data.\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "Transfer learning in image classification involves reusing a pre-trained model, typically trained on a large dataset like ImageNet, by fine-tuning it for a new, related task instead of training from scratch.​\n",
        "\n",
        "**How It Reduces Computational Costs :**\n",
        "Pre-trained models provide learned features (e.g., edges, textures) from earlier layers, allowing practitioners to freeze those layers and train only the final classification layers, which slashes training time from weeks to hours and requires less GPU/CPU power. This approach leverages billions of parameters already optimized, avoiding the high costs of processing massive datasets anew.​\n",
        "\n",
        "**Improving Performance with Limited Data :**\n",
        "With small datasets, models often overfit; transfer learning counters this by starting with robust, generalizable features that boost accuracy even on few samples, as the pre-trained backbone captures transferable visual hierarchies. Fine-tuning adapts these features to the target domain, yielding better generalization than random initialization, especially for tasks like medical imaging or custom object detection.​\n",
        "\n",
        "Common Approaches :     \n",
        "- Feature extraction: Freeze base layers, add new classifier; ideal for very limited data.\n",
        "\n",
        "- Fine-tuning: Unfreeze top layers gradually for domain adaptation; balances speed and customization.​"
      ],
      "metadata": {
        "id": "H0cyyHaz_rTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.5. Describe the role of residual connections in ResNet architecture. How do\n",
        "they address the vanishing gradient problem in deep CNNs?\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "Residual connections in ResNet architecture are shortcut pathways that add the input\n",
        "x\n",
        "x directly to the output of stacked layers\n",
        "F\n",
        "(\n",
        "x\n",
        ")\n",
        "F(x), forming the residual block output\n",
        "x\n",
        "+\n",
        "F\n",
        "(\n",
        "x\n",
        ")\n",
        "x+F(x), where the network learns the residual function\n",
        "F\n",
        "F rather than the full mapping.​\n",
        "\n",
        "**Role in ResNet :**\n",
        "\n",
        "These connections enable training of very deep networks (e.g., ResNet-152) by allowing identity mappings, where layers can learn perturbations around the input, simplifying optimization and improving feature reuse across depths. They create direct information highways, preserving spatial details and gradients throughout the network.​\n",
        "\n",
        "**Addressing Vanishing Gradients :**\n",
        "In deep CNNs, gradients diminish during backpropagation due to repeated multiplications by weights near 1, causing vanishing gradients that stall learning in later layers. Residual connections bypass layers via skip paths, ensuring constant gradient flow (at least 1 from the identity), which stabilizes training and allows deeper architectures without degradation. This results in better convergence and accuracy on tasks like ImageNet classification."
      ],
      "metadata": {
        "id": "x7ZOPSzi_rOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.6. Implement the LeNet-5 architectures using Tensorflow or PyTorch to\n",
        "classify the MNIST dataset. Report the accuracy and training time.\n",
        "\n",
        "Answer ->>"
      ],
      "metadata": {
        "id": "ygqDyric_rK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "# Define LeNet-5 architecture\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2)  # 32x32 input, pad=2 for same size\n",
        "        self.tanh1 = nn.Tanh()\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)  # input 14x14, output 10x10\n",
        "        self.tanh2 = nn.Tanh()\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5)  # input 5x5, output 1x1\n",
        "        self.tanh3 = nn.Tanh()\n",
        "\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.tanh4 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.tanh1(self.conv1(x)))\n",
        "        x = self.pool2(self.tanh2(self.conv2(x)))\n",
        "        x = self.tanh3(self.conv3(x))\n",
        "        x = x.view(-1, 120)\n",
        "        x = self.tanh4(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Data preparation with resizing to 32x32 as LeNet-5 expects\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32,32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1000, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LeNet5().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Testing accuracy\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "end_time = time.time()\n",
        "accuracy = 100 * correct / total\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "print(f'Training Time: {training_time:.2f} seconds')\n"
      ],
      "metadata": {
        "id": "S3ukUTFBEe20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.7.  Use a pre-trained VGG16 model (via transfer learning) on a small custom\n",
        "dataset (e.g., flowers or animals). Replace the top layers and fine-tune the model.\n",
        "Include your code and result discussion.\n",
        "\n",
        "Answer ->>"
      ],
      "metadata": {
        "id": "sGXj1ICjDcB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# -----------------------------\n",
        "# 1. DATA PREPROCESSING\n",
        "# -----------------------------\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    \"dataset/train\",         # <-- Change your path here\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    \"dataset/val\",          # <-- Change your path here\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "num_classes = train_gen.num_classes\n",
        "\n",
        "# -----------------------------\n",
        "# 2. LOAD PRETRAINED VGG16 MODEL\n",
        "# -----------------------------\n",
        "base_model = VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_shape=(224, 224, 3)\n",
        ")\n",
        "\n",
        "# Freeze layers initially\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# -----------------------------\n",
        "# 3. BUILD CUSTOM CLASSIFIER HEAD\n",
        "# -----------------------------\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(1e-3),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# -----------------------------\n",
        "# 4. TRAIN TOP LAYERS (FEATURE EXTRACTION)\n",
        "# -----------------------------\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 5. FINE-TUNE LAST FEW LAYERS OF VGG16\n",
        "# -----------------------------\n",
        "for layer in base_model.layers[-4:]:  # unfreeze last 4 conv layers\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(1e-5),  # lower LR for fine-tuning\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 6. FINAL EVALUATION\n",
        "# -----------------------------\n",
        "loss, accuracy = model.evaluate(val_gen)\n",
        "print(f\"\\nFinal Validation Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Bb7E9mVbEahO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.8. Write a program to visualize the filters and feature maps of the first\n",
        "convolutional layer of AlexNet on an example input image.\n",
        "\n",
        "Answer ->>"
      ],
      "metadata": {
        "id": "mEQgj9WsEoxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------------------\n",
        "# 1. Load Pretrained AlexNet\n",
        "# -----------------------------------------\n",
        "alexnet = models.alexnet(weights=models.AlexNet_Weights.IMAGENET1K_V1)\n",
        "alexnet.eval()  # inference mode\n",
        "\n",
        "# First convolution layer\n",
        "conv1 = alexnet.features[0]   # Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n",
        "\n",
        "# -----------------------------------------\n",
        "# 2. Visualize Filters of First Conv Layer\n",
        "# -----------------------------------------\n",
        "filters = conv1.weight.data.clone()\n",
        "\n",
        "# Normalize filters to [0,1] for visualization\n",
        "min_val = filters.min()\n",
        "max_val = filters.max()\n",
        "filters = (filters - min_val) / (max_val - min_val)\n",
        "\n",
        "# Plot filters\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "for i in range(1, 65):   # 64 filters\n",
        "    ax = fig.add_subplot(8, 8, i)\n",
        "    ax.imshow(filters[i-1].permute(1, 2, 0))\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"AlexNet Conv1 Filters\", fontsize=20)\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------------------\n",
        "# 3. Preprocess Input Image\n",
        "# -----------------------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ),\n",
        "])\n",
        "\n",
        "# Load your image\n",
        "img_path = \"sample.jpg\"   # <-- Put your own image here\n",
        "img = Image.open(img_path).convert(\"RGB\")\n",
        "input_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "# -----------------------------------------\n",
        "# 4. Pass Image Through First Conv Layer\n",
        "# -----------------------------------------\n",
        "with torch.no_grad():\n",
        "    feature_maps = conv1(input_tensor)\n",
        "\n",
        "# -----------------------------------------\n",
        "# 5. Visualize Feature Maps (Activation Maps)\n",
        "# -----------------------------------------\n",
        "fm = feature_maps.squeeze(0)\n",
        "\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "for i in range(1, 65):  # 64 feature maps\n",
        "    ax = fig.add_subplot(8, 8, i)\n",
        "    ax.imshow(fm[i-1].cpu().numpy(), cmap=\"gray\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"AlexNet Conv1 Feature Maps\", fontsize=20)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MtDQEi5IFC2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.9. Train a GoogLeNet (Inception v1) or its variant using a standard dataset\n",
        "like CIFAR-10. Plot the training and validation accuracy over epochs and analyze\n",
        "overfitting or underfitting.\n",
        "\n",
        "Answer ->>"
      ],
      "metadata": {
        "id": "a1p7VtEkE7k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# Inception Module (core of GoogLeNet)\n",
        "class Inception(nn.Module):\n",
        "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n",
        "        super(Inception, self).__init__()\n",
        "        # 1x1 branch\n",
        "        self.branch1 = nn.Conv2d(in_channels, ch1x1, kernel_size=1)\n",
        "\n",
        "        # 3x3 branch (with 1x1 reduction)\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, ch3x3red, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        # 5x5 branch (with 1x1 reduction)\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, ch5x5red, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(ch5x5red, ch5x5, kernel_size=5, padding=2)\n",
        "        )\n",
        "\n",
        "        # Pooling branch\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels, pool_proj, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = F.relu(self.branch1(x))\n",
        "        b2 = F.relu(self.branch2(x))\n",
        "        b3 = F.relu(self.branch3(x))\n",
        "        b4 = F.relu(self.branch4(x))\n",
        "        return torch.cat([b1, b2, b3, b4], 1)\n",
        "\n",
        "# Simplified GoogLeNet (Inception v1) for CIFAR-10 (32x32 images)\n",
        "class GoogLeNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(GoogLeNet, self).__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.inceptions = nn.Sequential(\n",
        "            Inception(192, 64, 96, 128, 16, 32, 32),      # 128 channels total\n",
        "            Inception(256, 128, 128, 192, 32, 96, 64),     # 480 channels total\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            Inception(480, 192, 96, 208, 16, 48, 64),      # 512 channels total\n",
        "            Inception(512, 160, 112, 224, 24, 64, 64),     # 512 channels total\n",
        "            Inception(512, 128, 128, 256, 24, 64, 64),     # 528 channels total\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            Inception(528, 256, 160, 320, 32, 128, 128),   # 832 channels total\n",
        "            Inception(832, 384, 192, 384, 48, 128, 128),   # 1024 channels total\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)           # 192 x 8 x 8\n",
        "        x = self.inceptions(x)     # 1024 x 1 x 1 (after adaptive pooling)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Data loading and preprocessing for CIFAR-10\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# Training setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GoogLeNet(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "# Training loop with accuracy tracking\n",
        "num_epochs = 50\n",
        "train_losses, train_accs = [], []\n",
        "val_accs = []\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for inputs, labels in trainloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(trainloader)\n",
        "    train_acc = 100. * correct / total\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_acc = 100. * correct / total\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f'Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%')\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f'\\nTraining completed in {training_time/60:.2f} minutes')\n",
        "print(f'Final Test Accuracy: {val_accs[-1]:.2f}%')\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_accs, label='Train Acc')\n",
        "plt.plot(val_accs, label='Val Acc')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('googlenet_cifar10_training.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N3zP7idvFSDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.10. You are working in a healthcare AI startup. Your team is tasked with\n",
        "developing a system that automatically classifies medical X-ray images into normal,\n",
        "pneumonia, and COVID-19. Due to limited labeled data, what approach would you\n",
        "suggest using among CNN architectures discussed (e.g., transfer learning with ResNet\n",
        "or Inception variants)? Justify your approach and outline a deployment strategy for\n",
        "production use.\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "**Recommended Approach: Transfer Learning with ResNet-50**\n",
        "\n",
        "For classifying medical X-ray images (normal, pneumonia, COVID-19) with limited labeled data, use transfer learning with ResNet-50. ResNet-50 outperforms Inception variants in medical imaging tasks (93.81% vs 91.76% accuracy on similar 3-class fundus classification) due to residual connections that enable stable training on small datasets without vanishing gradients.​\n",
        "\n",
        "**Key Benefits for Limited Data :**\n",
        "\n",
        "- Pre-trained ImageNet features capture edges/textures relevant to X-rays\n",
        "\n",
        "- Freeze early layers, fine-tune last 20-30% → 90%+ accuracy with 1-5k samples\n",
        "\n",
        "- Robust to class imbalance (COVID-19 often underrepresented)​\n",
        "\n",
        "**Implementation Strategy :**\n",
        "\n",
        "Core approach using PyTorch\n",
        "\n",
        "import torch\n",
        "\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "\n",
        "model = resnet50(pretrained=True)\n",
        "\n",
        "Modify for 3 classes + grayscale→RGB\n",
        "\n",
        "model.fc = nn.Linear(model.fc.in_features, 3)\n",
        "\n",
        "for param in model.parameters(): param.requires_grad = False\n",
        "\n",
        "for param in model.layer4.parameters(): param.requires_grad = True  #\n",
        "Fine-tune last block\n",
        "\n",
        "\n",
        "**Training Pipeline :**\n",
        "\n",
        "1. Data Prep: Resize X-rays to 224×224, convert grayscale→3 channels, heavy augmentation (rotation, brightness for X-ray variability)\n",
        "\n",
        "2. Two-Stage Training: Feature extraction (5 epochs), fine-tuning top layers (10-15 epochs, lr=1e-4)\n",
        "\n",
        "3. Metrics: F1-score per class (prioritize pneumonia/COVID recall), stratified k-fold validation\n",
        "\n",
        "**Production Deployment Strategy :**\n",
        "\n",
        "Healthcare AI Pipeline (MLOps):\n",
        "\n",
        "├── Data Pipeline: DICOM→PNG preprocessing, online augmentation\n",
        "\n",
        "├── Model Serving: FastAPI + TorchServe (50ms inference)\n",
        "\n",
        "├── Monitoring: Class drift detection, accuracy per hospital scanner\n",
        "\n",
        "├── Explainability: GradCAM heatmaps for clinician trust\n",
        "\n",
        "└── Regulatory: Model cards, FDA 510(k) pathway documentation\n",
        "\n",
        "**Deployment Architecture :**\n",
        "\n",
        "\n",
        "step-1 : PACS System → Preprocessing Microservice → ResNet-50 (GPU pods)\n",
        "\n",
        "    \n",
        "step-2 : [Inference: 99.9% uptime] → GradCAM → Radiologist Dashboard\n",
        "    \n",
        "step-3 : Model Registry (MLflow) ← Retraining Pipeline (new X-rays)"
      ],
      "metadata": {
        "id": "8up7_H4ZFYSb"
      }
    }
  ]
}